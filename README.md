# AlphaEx
The AlphaEx (Alpha Experiment) is a python toolkit helps you run a large number of experiments easily and efficiently.

With AlphaEx, you can:
1. Run thousands of experiments on multiple computer clusters automatically, so that you can squeeze the most of your computation hardware.
2. Sweep parameters in a deadly simple yet effective way. Just define a json file and do your sweeps with one click.
3. Merge the log data generated by your experiments and make plots using the data in flexible ways.

The above 3 functions are implemented in 3 self-contained python scripts
`submitter.py`, `sweeper.py`, and `plotter.py`.

Notice:
1. the sweeper and plotter can be used in any machine. But submitter
currently is only compatible with slurm, so make sure you have access to
at least one remote cluster which has slurm installed. For example, I
have an account on compute canada, so I can use clusters including
cedar, mp2, etc.
2. The submitter current only support cpu jobs. I hope to extend that to support gpu jobs but this might take a while. If you are interested in doing that let's discuss about this.

To test these 3 modules, run
`python test/test_sweeper.py`, `python test/test_plotter.py` and
`python test/test_submitter.py` (submitter needs to be configured first
with you own setting. test/test_submitter.py is an example)
.

## Submitter
### How to use it
To use submitter, you need to first have automatic ssh access to remote clusters,
so that every time when you ssh to a remote cluster, just type in `ssh clustername`
without entering the full url, your username and password.

The next 3 steps help you do this. In your own laptop's home directory:

1. `ssh-keygen`
2. `ssh-copy-id <username>@<cluster url>`
3. Add the cluster information in .ssh/config
```
Host *
    AddKeysToAgent yes
    IdentityFile ~/.ssh/id_rsa

Host <cluster nickname>
    HostName <cluster url>
    User <username>
```
Next time when you want to add a new cluster, just do step 2 and step 3.

Now you can use submitter. test/test_submitter.py is a simple example.

```
from experimenter.submitter import Submitter


def test_submitter():
	clusters = [
		{
			'name': 'mp2',
			'capacity': 200,
			'project_root_dir': '/home/yiwan/projects/def-sutton/yiwan/experimenter',
			'script_path': 'test/submit.sh'
		},
		{
			'name': 'cedar',
			'capacity': 100,
			'project_root_dir': '/home/yiwan/projects/def-sutton/yiwan/experimenter',
			'script_path': 'test/submit.sh'
		},
	]
	submitter = Submitter(clusters, 1000)
	submitter.submit()


if __name__ == '__main__':
	test_submitter()
```

Submitter class instantiates with a list of dictionary and a integer. The list contains the information of
all clusters that you want to run your experiment on and the integer is the total number of jobs.

Each dictionary gives information of a cluster:

`name`: the nickname of your remote cluster, it should be defined in .ssh/config.

`capacity`: maximum number of jobs you want to run in that cluster

`project_root_dir`: the root directory for your project

`script_path`: the path of the slurm array job script in the remote cluster that you want to submit

test/submit.sh is an example of the such script:

```
#!/bin/bash

#SBATCH --time=02:55:00
#SBATCH --mem-per-cpu=1G
#SBATCH --job-name submit.sh
#SBATCH --output=output/submit_%j.txt
#SBATCH --error=error/submit_%j.txt

export OMP_NUM_THREADS=1
mkdir -p output
mkdir -p error

sleep $((SLURM_ARRAY_TASK_ID / 10))
```

In this simple example, each job is just sleeping for some time. The sleeping time depends on the SLURM_ARRAY_TASK_ID.
Each job has its own SLURM_ARRAY_TASK_ID which will be assigned by submitter.
Refer to the user manual of slurm if you don't understand the above script.

Now run `python test/test_submitter.py` in your local machine, it will submit 1000 jobs to cluster mp2 and cedar.

### How it works
In the above example, when you run test_submitter.py, the submitter will submit array jobs with array indices 1-200 to cluster mp2, and submit array jobs 201-300 to cluster cedar.
After that, it will periodically check whether there is any submitted job finishes.
If there is any, submitter will submit the same number of new jobs as the finished ones, until all 1000 jobs are submitted.